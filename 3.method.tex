\section{Method}
\subsection{Background}
To test our hypothesis in a realistic setting,  we designed a 2(web vs. chatbot)*2(regular format vs. highlighed experts titles) between-subjects experiment and conduct it in several universities and colleges in mainland China from April 14, 2022 to April 25, 2022.
For the purpose of promoting people's participation, we prepared a lucky draw session. Participants who complete the experiment will have a chance to win one of thirty 100 RMB e-voucher prizes.
Our study received ethics approval(human, non-clinical) from the Research Ethics Committee of the Hong Kong Baptist University on March 17, 2022.
\subsection{Participants}
A total of 708 individuals registered for our experiment. We recruited these volunteers from several universities using social media and bulletin boards, and we received informed consent from each of them.
During the experiment, about 25.7\% of the participants dropped out, with similar drop-out rates among all four conditions. We ended up with 526 initially completed responses.
To ensure the quality of the data, we employ three additional filtering  procedures: 
1)Two instructed response items are set to identify inattentive respondents in the pre-test and post-test, respectively. 
Participants(n=196) who failed in any of the attitude check questions are filtered.
2)The 3*IQR(Interquartile Range) is used as a threshold to detect extreme outliers of the completion time. 
Three participants were excluded because they took an extraordinarily long time. 
3)Participants(n=19) who answered all post-test questions with the same response were filtered.
Thus, 308 valid responses were retained for analysis. The final sample of participates consisted of 222 females(72.1\%), 85 males(27.6\%) and 1 other or prefer not to say(0.3\%).
Their average age was 22 years old(M=21.71,SD=3.85), with 254 pursing bachelor's degrees, and this number is 38 for master's degrees, 6 for PhDs and 10 for others. 
Our four groups are roughly the same size and the fisher's exact test with Monte Carlo simulation found no significant demographic differences between there four conditions(gender: p=0.12,edu: p=0.73).
This indicates that the sample is balanced for each condition.

\subsection{Experimental Design}
\subsubsection{Interaction Type} 
Two interaction types, the web and the chatbot, were used in this experiment to present the fact-checking results.
The web condition references the layout and style of many fact-checking websites, which users can browse fact-checking content directly.
The chatbot is designed as a Q\&A style, with dubious claims organized as a list of buttons in the form of questions( e.g., Can staying up late cause pneumothorax?), and the user engaging with the chatbot by clicking on the question to get the feedback.
The fact-check results presented under both conditions are identical.

\subsubsection{Emphasize Expert Title}
We display the title of expert in both interaction types, compared to the formal style, the highlighting format bolded the font of the expert's title and added an avatar before it.
Thus, the only difference between the formal and highlighting conditions was the boldness of the expert title and the avatar.

\subsubsection{Stimuli and Correction Materials}
The topic of the "fake news" employed in the experiment was the negative effects of staying up late on health, which was the top concern of young adults identified in the pilot survey.
Based on this topic, we borrowed some fact-checking results from a well-known Tencent fact-checking platform in China and prepared an article containing four false claims.
The article is based on a real news story and intentionally added four fact-based false statements to simulate real-world instances of health-related fake news.
The made-up verification report follows the standard fact-checking report format, with a short take at the top of the report, followed by a detailed  explanation  or justification of each claim that was verified.
The reason we don't use fact-checked reports and checked articles directly from this platform is to avoid the impact on users from previous exposure of this content.

\subsubsection{Procedure}
Our online experiment consisted of three main steps: a pre-test questionnaire, reading the article and the fact-checking report, and a post-test questionnaire.
Participants were required to read a brief introduction before the experiment, followed by the pre-test in which they answered a series of questions about their pre-knowledge and demographics.
After that, participants were given this news article about staying up late and were randomly assigned to one of the four conditions to read the fact-checking results about this news.
When participants completed their interaction with the fact-checking interface, they were asked to provide the evaluation and feedback of this fact-checking tool by answering the post-test questionnaire.
Finally they were required to answer six questions about actual effectiveness, which were the same as the questions about the pre-knowledge in the pre-test.
Before that, participants were asked to complete a breif distractor task of finding 16 differences between two pictures in two minutes. This design is consistent with previous studies~\cite{swire2017role,ecker2015he,walter2021unchecked,amazeen2018correcting}
It took participants on average 12 minutes to complete the whole study.

\subsection{Measures}
We measure the impact of the tool's design on users in terms of the effectiveness of the correction, the levels of cognitive effort expended, and the intention to use and check.
All questions were adapted from existing studies and translated into Mandarin Chinese. 
To ensure the quality of the translations we employed a parallel translation method~\cite{harkness2004survey} in which two researchers of this study independently translated the questionnaire in parallel and referred the items that could not be agreed upon to the third researcher for adjudication.

\subsubsection{Perceived effectiveness.}
To assess the participants' perceived effectiveness of the fact-checking result, we used six 5-point semantic differential items developed by Dillard et al.(2007)~\cite{dillard2007does}.
Participants were asked to evaluate the fact-checking report using these word pairs,ranging from low effectiveness (1) to high effectiveness (5) included: "Not convincing-Convincing", "Not Believable-Believable", "Not sensible-Sensible", "Foolish-Wise", "Wrong-Right" and "Unimportant-Important".
The Cronbach’s alpha was 0.905 and average variance extracted(AVE) was 0.617.

\subsubsection{Actual effectiveness.}
To determine the actual effectiveness of fact-checking, participants were asked to report their opinions on six statements in the pre-test and post-test, respectively.
They can label each statement as true, false or I don't know.
These statements were extracted from the fact-checking report, and to balance out the set, three statements were set to incorrect(myths) and the other three were correct(facts).
Previous studies shows that the corrective effect of fact-checking can be influenced by many factors, and belief or attitude change can be considered a valid measure of the actual effectiveness~\cite{swire2017role,fennis2015psychology}.
We employed a pretest-posttest design to determine the actual effectiveness by comparing the change in number of statements correctly rated before and after the manipulation. Selecting the "I don't know option" is also counted as an incorrect judgment
The test of actual effectiveness was assigned at the beginning and end of the experiment to ensure sufficient interval length.

\subsubsection{Effort Expectancy.}
Effort expectancy refer to the ease of using these interfaces to verify facts.
we asked participates to report their level of agreement using a 5-point Likert-type scale for four statements designed by~\cite{venkatesh2003user}.
These four items included:"My interaction with this "Hawkeye" Fact-Checking Service page is clear and understandable,"
"It is easy for me to become skillful at using this "Hawkeye" Fact-Checking Service,"
"I found this "Hawkeye" Fact-Checking Service easy to use," and
"Learning to operate this "Hawkeye" Fact-Checking Service is not easy for me."
We use the confirmatory factor analysis (CFA) to investigate the factorial structure of all items in the post-test questionnaire and removed item1 and item4 of effort expectancy due to the facter loading < 0.5.
Finally the Cronbach’s alpha was  0.773 and average variance extracted(AVE) was 0.631.

\subsubsection{Intention to use.}
We measured intention to use based on the three validated questions(Cronbach alpha: 0.870; AVE: 0.691) from the Unified Theory of Acceptance and Use of Technology(UTAUT) model~\cite{venkatesh2003user}.
The three items are "I intend to use this "Hawkeye" Fact-checking Service in the future,"
"I predict I will use this "Hawkeye" Fact-checking Service in the future," and 
"I plan to use this "Hawkeye" Fact-checking Service in the future." 

\subsubsection{Intention to check.}
Our 3-item measure for the intention to check were adapted from a prior literature and replied on the 5-point Likert scale (1=strongly disagree, 5=strongly agree).
These statements included: "In the future, I will actively fact-check health-related news," "In the future, I will not try to verify the truthfulness of health-related news," and "In the future, I will earnestly attempt to find out the truth about health-related news."
We dropped the second statement since the original three items have low internal consistency (Cronbach’s alpha =0.617),and the Cronbach’s alpha was 0.727 and AVE was 0.571 after correction.





