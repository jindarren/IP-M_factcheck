\section{Results}
In this study, we conducted an empirical investigation to evaluate the impact of interaction types(web v.s. chatbot) and correction formats(highlighting expert titles or not) on fact-checking effectiveness(RQ1), cognitive effort(RQ2) and further analyzed which conditions help increase users' intention to use. 
For ease of description, we use \textbf{Interaction*Format} to denote our experimental conditions. 
"Interaction" can be classified as "web" and "chatbot", which refer to using the webpage or the chatbot to obtain fact-checking results, respectively.  
"Format" can be "Regular" or "Highlighted"(Highlighted expert titles).

We used the two-way ANOVA to examine the main effects and interactions.
Before that, a Shapiro-Wilk normality test was conducted to validate the normality assumption. The results showed that our data violated this assumption. 
Therefore, we used the Aligned Rank Transform(ART) procedure from the R-language Artool package to conduct the nonparametric factorial ANOVA.

\begin{table}[width=.9\linewidth,cols=5,pos=h]
    \caption{Descriptive Analysis of all measured dependent validate}\label{tbl1}
    \label{tab:descriptive}
    \begin{tabular*}{\tblwidth}{@{} LCCCC@{} }
    \toprule
    \textbf{Dependent}&\textbf{Web*Regualr}&\textbf{Web*Expert}&\textbf{Chatbot*Regualr} & \textbf{Chatbot*Expert}\\
    \textbf{variable}&(N=74)&(N=72)&(N=78)&(N=84)\\
     & Mean (SD) & Mean (SD) & Mean (SD) & Mean (SD) \\
    \midrule
    \textbf{Perceived Effectiveness}&\textbf{4.29 (0.63)} & 4.04 (0.75) & 4.11(0.73) & 4.28(0.58)\\
    \textbf{Actual Effectiveness}&1.51 (1.47) & 1.57 (1.47) & 1.79(1.22) & \textbf{1.82(1.44)}\\
    \textbf{Cognitive Effort}&3.86 (0.73) & 3.68 (0.76) & 4.15(0.67) & \textbf{4.17(0.74)}\\
    \textbf{Intention to Use}&3.98 (0.86) & 3.69 (0.82) & 3.93(0.68) & \textbf{4.00(0.67)}\\
    \bottomrule
    \end{tabular*}
    \end{table} 

    \begin{figure*}
        \centering
          \includegraphics[width=\textwidth,height=4.6in]{figs/fig_main_interaction.png}
        \caption{The results of nonparametric factorial ANOVA.The interaction type has a significant main effect on (a) cognitive effects and a weak main effect on (b) objective effectiveness. 
        The chatbot is more likely to reduce users' cognitive effort and improve the actual effectiveness of correction.
        Two significant intention effects exist between interaction type and currection format on (c) perceived effectiveness and (d) intention to use.}
        \label{FIG:interaction}
    \end{figure*}

\subsection{Evaluation of Currection Effectiveness}
To comprehensively measure the correction effectiveness, we evaluated the experimental conditions from both subjective and objective aspects.
The results revealed an interaction effect in subjective effectiveness, while there was no significant difference in actual effectiveness.

\subsubsection{Perceived Effectiveness}
The 2*2 ANOVA shows a significant interaction effect between interaction type and format on perceived effectiveness, F(1, 323)=6.31, p<0.05, $\eta^{2}$=0.020.
We can observe that when participants use the web for fact-checking, the emphasis on expert titles decreases their perceived effectiveness.
However, there is an opposite trend when using the chatbot(see Figure~\ref{FIG:interaction}(d)).
Besides, the Chatbot*Highlighted design has a similar level of perceived effectiveness with the Web*Regualr(Table~\ref{tab:descriptive}).

\subsubsection{ Objectiveness Effectiveness}

Table~\ref{tab:descriptive} shows the participants' performance on the objective effectiveness questions. 
Participants answered the same questions in both tests, and we measured objective effectiveness by the increased number of correct answers to objective questions.
It can be seen that participants had better performance in the chatbot condition, while there was no significant effect on whether or not the expert title was highlighted.
In the web condition, participants' correct responses increased by 1.51 and 1.57 in the "Regular" and the "Highlighted" conditions, while in the chatbot condition, the increases were 1.79 and 1.82, respectively.
The results of two-way ANOVA show a weak significant main effect on the interaction type.

\subsection{Evaluation of Users' Intention}
We examine the impact of the tool on user's behavioral tendencies from both their intention to use this tool and their intention to fact-check it in the future.
Before that we measured the cognitive effort of users in different experimental conditions.
As mentioned before, people are not willing to spend extra cognitive effort to verify the authenticity of information. If the design can help users reduce the cognitive effort, this may have a positive impact on their behavioral tendencies, which is the reason we measured this variable.
There, higher values of the variable indicate easier of use and less cognitive effort.

\subsubsection{Cognitive Effort}
Analysis of cognitive effort yields a significant main effect of interaction type, F(1, 323)=20.239, p < 0.001, $\eta^{2}$ = 0.059, indicating that participants expend less cognitive effort to check facts when using chatbots compared to using the web(see Figure~\ref{FIG:interaction}(a)). 
The results do not show a significant main effect of format or an interaction effect.

\subsubsection{Intention to Use}
The main effects of interaction type and format are not found, but the interaction effect is significant, F (1, 323) = 5.310, p < 0.05, $\eta^{2}$ =0.022. 
Participants in the highlighted format reported lower intention to useÂ than in the normal format condition when they use the web, but this effect disappears under the chatbot condition(see Figure~\ref{FIG:interaction}(b)).
There is no significant difference between the regular and highlighted formats when the chatbot is applied. Besides, the chatbot*highlighted design leads to the highest intention to use(Mean=3.993,SD=0.680).
